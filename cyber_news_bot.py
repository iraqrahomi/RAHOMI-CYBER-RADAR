# -*- coding: utf-8 -*-
# cyber_news_bot.py â€” Ø§Ù„Ø±Ø§Ø¯Ø§Ø± Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ (Inline Buttons + Arabic/English + Icons + Hashtags)
import os, re, time, hashlib, sqlite3, requests, sys
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import feedparser
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© =====
TZ = timezone(timedelta(hours=+3))  # Asia/Baghdad
DB_PATH = "cyber_news.db"
USER_AGENT = "Mozilla/5.0 (compatible; RahomiCyberRadar/2.0)"

TG_TOKEN = os.getenv("TG_TOKEN", "")
TG_CHAT_ID = os.getenv("TG_CHAT_ID", "")

# ===== Ø§Ù„Ù…ØµØ§Ø¯Ø± =====
AR_SOURCES = {
    "CybersecurityCast-Ø«ØºØ±Ø§Øª": {"type":"scrape","url":"https://cybersecuritycast.com/category/%D8%AB%D8%BA%D8%B1%D8%A7%D8%AA-%D8%A3%D9%85%D9%86%D9%8A%D8%A9/","lang":"ar"},
    "CYBRAT": {"type":"scrape","url":"https://cybrat.net/","lang":"ar"},
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-Ø£Ù…Ù†-Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ": {"type":"scrape","url":"https://www.alarabiya.net/technology/cybersecurity-privacy","lang":"ar"},
    "NCSC-JO": {"type":"scrape","url":"https://ncsc.jo/","lang":"ar"},
}
EN_SOURCES = {
    "TheHackerNews": {"type":"rss","url":"https://feeds.feedburner.com/TheHackersNews","lang":"en"},
    "SecurityWeek": {"type":"rss","url":"https://www.securityweek.com/feed/","lang":"en"},
    "DarkReading": {"type":"rss","url":"https://www.darkreading.com/rss.xml","lang":"en"},
}

KEYWORDS_AR = ["Ø«ØºØ±Ø©","Ø«ØºØ±Ø§Øª","Ø§Ø®ØªØ±Ø§Ù‚","ØªØµÙŠØ¯","ÙØ¯ÙŠØ©","Ø§Ø¨ØªØ²Ø§Ø²","ØªØ¬Ø³Ø³","CVE","Ø²ÙŠØ±Ùˆ","Zero-Day","RCE","Ø¨ÙˆØª Ù†Øª","Ø¨Ø±Ù…Ø¬ÙŠØ©","ØªØ³Ø±ÙŠØ¨","Ø­Ø±Ø¬Ø©","Ù‡Ø¬ÙˆÙ…"]
KEYWORDS_EN = ["CVE","Zero-Day","RCE","exploit","ransomware","breach","malware","phishing","data leak","privilege escalation","attack"]

# ===== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====
def now_str(): 
    return datetime.now(TZ).strftime("%Y-%m-%d %H:%M")

def h(s: str) -> str: 
    return hashlib.sha256((s or "").encode("utf-8")).hexdigest()

def html_escape(s: str) -> str: 
    return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def short_for_display(url: str, maxlen: int = 60) -> str:
    try:
        p = urlparse(url)
        disp = f"{p.netloc}{p.path}"
    except Exception:
        disp = url or ""
    return disp if len(disp) <= maxlen else disp[:maxlen-3] + "..."

UTM_KEYS = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","utm_id"}
STRIP_KEYS = UTM_KEYS | {"fbclid","gclid","mc_cid","mc_eid","igsh","si"}

def normalize_url(u: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·: Ø­Ø°Ù UTM/fbclid ÙˆØ£Ø¬Ø²Ø§Ø¡ fragmentØŒ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ ÙˆØ®ÙØ¶ Ø§Ù„Ø­Ø±ÙˆÙØŒ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ø³Ù„Ø§Ø´ Ø§Ù„Ø²Ø§Ø¦Ø¯"""
    u = (u or "").strip()
    if not u:
        return ""
    p = urlparse(u)
    scheme = (p.scheme or "https").lower()
    netloc = (p.netloc or "").lower()
    path = re.sub(r"/+$", "", p.path or "")
    q = [(k,v) for (k,v) in parse_qsl(p.query, keep_blank_values=False)
         if k.lower() not in STRIP_KEYS and not k.lower().startswith("utm_")]
    query = urlencode(sorted(q))
    return urlunparse((scheme, netloc, path, "", query, ""))

def ensure_db():
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("""
      CREATE TABLE IF NOT EXISTS items (
        id TEXT PRIMARY KEY,           -- sha256(url_norm)
        source TEXT,
        title TEXT,
        url TEXT,                      -- Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„Ø²Ø±
        url_norm TEXT,                 -- Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø·Ø¨Ù‘Ø¹ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
        published TEXT,
        lang TEXT,
        sent_at TEXT                   -- ÙˆÙ‚Øª Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…
      )
    """)
    # ØªØ±Ù‚ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ø³Ø®Ø© Ù‚Ø¯ÙŠÙ…Ø©
    for col in ("url_norm","sent_at"):
        try:
            cur.execute(f"ALTER TABLE items ADD COLUMN {col} TEXT")
        except sqlite3.OperationalError:
            pass
    # ÙÙ‡Ø±Ø³ ÙØ±ÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø·Ø¨Ù‘Ø¹
    cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS uniq_url_norm ON items(url_norm)")
    con.commit(); con.close()

def exists_by_url(url: str) -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… url_norm"""
    un = normalize_url(url)
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("SELECT 1 FROM items WHERE url_norm=?", (un,))
    ok = cur.fetchone() is not None
    con.close(); 
    return ok

def save_item(source, title, url, lang, published=None):
    if not published: 
        published = now_str()
    un = normalize_url(url)
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("""INSERT OR IGNORE INTO items
        (id, source, title, url, url_norm, published, lang, sent_at)
        VALUES (?,?,?,?,?,?,?,NULL)""",
        (h(un), source, (title or "").strip(), url, un, published, lang))
    con.commit(); con.close()

def http_get(url):
    resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=25)
    resp.raise_for_status(); return resp

def match_keywords(text: str, lang: str) -> bool:
    t = (text or "").lower()
    keys = KEYWORDS_AR if lang == "ar" else KEYWORDS_EN
    return any(k.lower() in t for k in keys)

# ØªØµÙ†ÙŠÙ Ø£ÙŠÙ‚ÙˆÙ†Ø© + Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª
def classify_icon_and_tags(title: str, lang: str):
    t = (title or "").lower()
    icon = "ğŸ›¡ï¸"; tags = []
    checks = [
        (["Ø«ØºØ±Ø©","cve","rce","exploit","bug","vulnerab"], "ğŸ", ["#Ø«ØºØ±Ø§Øª","#Vulnerability"]),
        (["ÙØ¯ÙŠØ©","ransom"], "ğŸ’°", ["#ÙØ¯ÙŠØ©","#Ransomware"]),
        (["Ù‡Ø¬ÙˆÙ…","attack","malware","trojan","rat"], "ğŸ’¥", ["#Ù‡Ø¬ÙˆÙ…","#CyberAttack"]),
        (["ØªØ¬Ø³Ø³","spy","phishing"], "ğŸ‘ï¸", ["#ØªØ¬Ø³Ø³","#Phishing"]),
        (["ØªØ³Ø±ÙŠØ¨","leak","breach"], "ğŸ§©", ["#ØªØ³Ø±ÙŠØ¨","#Breach"]),
        (["ØªØ­Ø¯ÙŠØ«","patch","fix"], "ğŸ› ï¸", ["#ØªØ­Ø¯ÙŠØ«","#Patch"]),
    ]
    for keys, ic, tg in checks:
        if any(k in t for k in keys):
            icon = ic; tags = tg; break
    tags = sorted(tags, key=lambda x: 0 if any("\u0600" <= ch <= "\u06FF" for ch in x) else 1)
    return icon, " ".join(tags[:2])

# ===== RSS =====
def pull_rss(name, url, lang):
    added = 0
    feed = feedparser.parse(url)
    for e in feed.entries:
        title = (e.get("title") or "").strip()
        link_raw = (e.get("link") or e.get("id") or "").strip()
        if not title or not link_raw: 
            continue
        # ÙÙ„ØªØ±Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        text = f"{title} {e.get('summary','')}"
        if not match_keywords(text, lang): 
            continue
        # Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø·Ø¨Ù‘Ø¹
        if exists_by_url(link_raw): 
            continue
        pub = e.get("published", "") or e.get("updated","") or now_str()
        save_item(name, title, link_raw, lang, pub); added += 1
    return added

# ===== Scrapers Ø¨Ø³ÙŠØ·Ø© =====
def scrape_cybersecuritycast(url):
    soup = BeautifulSoup(http_get(url).text, "html.parser")
    posts=[]
    for a in soup.select("article a[href]"):
        href_raw = (a.get("href","") or "").strip()
        title = (a.get_text() or "").strip()
        if not href_raw or not title or len(title) <= 10:
            continue
        posts.append((title, href_raw))
    return posts

def scrape_cybrat(url):
    soup = BeautifulSoup(http_get(url).text, "html.parser")
    posts=[]
    for a in soup.select("a[href]"):
        href_raw = (a.get("href","") or "").strip()
        text = (a.get_text() or "").strip()
        if href_raw and text and re.search(r"/\d{4}/\d{2}/\d{2}/", href_raw):
            posts.append((text, href_raw))
    return posts

def scrape_alarabiya(url):
    soup = BeautifulSoup(http_get(url).text, "html.parser")
    posts=[]
    for a in soup.select("a[href]"):
        href_raw = (a.get("href","") or "").strip()
        text = (a.get_text() or "").strip()
        if href_raw.startswith("https://www.alarabiya.net") and len(text) > 15:
            posts.append((text, href_raw))
    return posts

def scrape_ncsc(url):
    soup = BeautifulSoup(http_get(url).text, "html.parser")
    posts=[]
    for a in soup.select("a[href]"):
        href_raw = (a.get("href","") or "").strip()
        text = (a.get_text() or "").strip()
        if href_raw.startswith("https://ncsc.jo/") and len(text) > 10:
            posts.append((text, href_raw))
    return posts

SCRAPERS = {
    "CybersecurityCast-Ø«ØºØ±Ø§Øª": scrape_cybersecuritycast,
    "CYBRAT": scrape_cybrat,
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-Ø£Ù…Ù†-Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ": scrape_alarabiya,
    "NCSC-JO": scrape_ncsc,
}

# ===== Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± =====
def collect():
    ensure_db()
    total = 0
    # RSS Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
    for name, cfg in EN_SOURCES.items():
        if cfg["type"] == "rss":
            try:
                total += pull_rss(name, cfg["url"], cfg["lang"])
            except Exception as e:
                print("[WARN-RSS]", name, str(e))
            time.sleep(0.8)
    # Scraping Ø¹Ø±Ø¨ÙŠ
    for name, cfg in AR_SOURCES.items():
        if cfg["type"] != "scrape": 
            continue
        try:
            items = SCRAPERS[name](cfg["url"]); added = 0
            for title, link_raw in items:
                if not title or not link_raw: 
                    continue
                if not match_keywords(title, cfg["lang"]): 
                    continue
                if exists_by_url(link_raw): 
                    continue
                save_item(name, title, link_raw, cfg["lang"]); added += 1
            total += added; print(f"[OK] {name}: +{added}")
        except Exception as e:
            print("[WARN-SCRAPE]", name, str(e))
        time.sleep(1.2)
    return total

# ===== Ø¥Ø±Ø³Ø§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… =====
def tg_send_text(text):
    if not TG_TOKEN or not TG_CHAT_ID:
        print("[WARN] TG creds not set"); return False
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    data = {"chat_id": TG_CHAT_ID, "text": text, "parse_mode": "HTML", "disable_web_page_preview": True}
    r = requests.post(url, json=data, timeout=25)
    try:
        ok = r.json().get("ok", False)
    except Exception:
        ok = r.status_code == 200
    if not ok:
        print("[TG ERR TEXT]", r.text[:300])
    return ok

def tg_send_with_button(title_html, url_button):
    if not TG_TOKEN or not TG_CHAT_ID:
        print("[WARN] TG creds not set"); return False
    api = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    payload = {
        "chat_id": TG_CHAT_ID,
        "text": title_html,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
        "reply_markup": {
            "inline_keyboard": [[{"text": "ğŸŒ Ø§Ù„Ù…ØµØ¯Ø±", "url": url_button}]]
        }
    }
    r = requests.post(api, json=payload, timeout=25)
    try:
        ok = r.json().get("ok", False)
    except Exception:
        ok = r.status_code == 200
    if not ok:
        print("[TG ERR BTN]", r.text[:300])
    return ok

def build_item_html(src, title, url, lang):
    icon, tags = classify_icon_and_tags(title, lang)
    safe_title = html_escape(title)
    # Ù†Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† + Ø§Ù„Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª ÙÙ‚Ø·ØŒ ÙˆØ§Ù„Ø²Ø± ÙŠØ­Ù…Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
    text = f"{icon} <b>[{html_escape(src)}]</b> {safe_title}\n<i>{tags}</i>"
    return text, url

# ÙŠØ±Ø³Ù„ ØºÙŠØ± Ø§Ù„Ù…ÙØ±Ø³ÙÙ„ ÙÙ‚Ø· Ø«Ù… ÙŠØ¹Ù„Ù‘Ù… sent_at Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
def make_and_push_digest(limit=14, pause=0.6):
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("""
        SELECT rowid, source, title, url, published, lang
        FROM items
        WHERE sent_at IS NULL
        ORDER BY
          CASE WHEN published GLOB '____-__-__ *__:*' THEN published ELSE '' END DESC,
          rowid DESC
        LIMIT ?
    """, (limit,))
    rows = cur.fetchall()

    if not rows:
        tg_send_text(f"Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ âœ… â€” {now_str()}")
        con.close()
        return

    ar_rows = [r for r in rows if r[5] == "ar"]
    en_rows = [r for r in rows if r[5] == "en"]

    head = f"âš¡ï¸ <b>Ø®Ù„Ø§ØµØ© Ø§Ù„Ø±Ø§Ø¯Ø§Ø± Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ</b> â€” {now_str()}"
    tg_send_text(head); time.sleep(pause)

    if ar_rows:
        tg_send_text("<b>Ø¹Ø±Ø¨ÙŠ</b>"); time.sleep(pause)
        for rowid, src, title, url, pub, lang in ar_rows:
            text_html, btn_url = build_item_html(src, title, url, lang)
            if tg_send_with_button(text_html, btn_url):
                cur.execute("UPDATE items SET sent_at=? WHERE rowid=?", (now_str(), rowid))
                con.commit()
            time.sleep(pause)

    if en_rows:
        tg_send_text("<b>English</b>"); time.sleep(pause)
        for rowid, src, title, url, pub, lang in en_rows:
            text_html, btn_url = build_item_html(src, title, url, lang)
            if tg_send_with_button(text_html, btn_url):
                cur.execute("UPDATE items SET sent_at=? WHERE rowid=?", (now_str(), rowid))
                con.commit()
            time.sleep(pause)

    con.close()

# ===== Ø§Ù„ØªØ´ØºÙŠÙ„ =====
if __name__ == "__main__":
    ensure_db()
    added = collect()
    print("[DONE] new items:", added)
    # Ø¥Ø°Ø§ Ù…Ø§ÙƒÙˆ Ø¬Ø¯ÙŠØ¯ØŒ ØªÙ‚Ø¯Ø± ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¥Ø±Ø³Ø§Ù„:
    # if added == 0: sys.exit(0)
    make_and_push_digest(limit=14)
